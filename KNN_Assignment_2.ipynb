{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec27b06",
   "metadata": {},
   "source": [
    "#### Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f501f48",
   "metadata": {},
   "source": [
    "The main difference between the Euclidean distance metric and the Manhattan distance metric in K-nearest neighbors (KNN) is how they measure the distance between data points in the feature space:\n",
    "\n",
    "Euclidean Distance:\n",
    "\n",
    "Euclidean distance, also known as L2 distance, calculates the straight-line distance between two points in a Euclidean space.\n",
    "It is calculated as the square root of the sum of squared differences between corresponding coordinates of two points.\n",
    "Mathematically, for two points (x1, y1) and (x2, y2) in a 2D space, the Euclidean distance is given by:\n",
    "D = âˆš((x2 - x1)^2 + (y2 - y1)^2)\n",
    "Manhattan Distance:\n",
    "\n",
    "Manhattan distance, also known as L1 distance or city block distance, measures the distance between two points by summing the absolute differences of their coordinates.\n",
    "It gets its name from the idea of traveling through a grid-like city block layout where you can only move horizontally or vertically.\n",
    "Mathematically, for two points (x1, y1) and (x2, y2) in a 2D space, the Manhattan distance is given by:\n",
    "D = |x2 - x1| + |y2 - y1|\n",
    "The choice of distance metric can significantly affect the performance of a KNN classifier or regressor:\n",
    "\n",
    "Sensitivity to Scale:\n",
    "\n",
    "Euclidean distance considers both the magnitude and direction of differences between data points. It is sensitive to the scale of the features, meaning that features with larger numeric values may dominate the distance calculations.\n",
    "Manhattan distance, on the other hand, only considers the absolute differences, making it less sensitive to feature scaling. It can be a better choice when the features have different units or scales.\n",
    "Impact on Decision Boundaries:\n",
    "\n",
    "The different ways of measuring distance can lead to different decision boundaries in the feature space.\n",
    "Euclidean distance tends to create circular or spherical decision boundaries because it considers the radial distance.\n",
    "Manhattan distance tends to create square or hyper-rectangular decision boundaries because it only considers horizontal and vertical movements.\n",
    "Depending on the distribution of data, one distance metric may be more suitable than the other. For example, in cases where data clusters are elongated or have irregular shapes, Manhattan distance might perform better.\n",
    "Computational Complexity:\n",
    "\n",
    "Manhattan distance is computationally less intensive to calculate because it involves simple absolute differences, while Euclidean distance requires taking square roots.\n",
    "If computational efficiency is a concern, Manhattan distance may be preferred in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca09b2",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69211562",
   "metadata": {},
   "source": [
    "- The value of K can be chosen by taking the square root of 'N',where 'N' is the no. of samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c1780",
   "metadata": {},
   "source": [
    "#### Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c73ac",
   "metadata": {},
   "source": [
    "The choice of distance metric in a K-nearest neighbors (KNN) classifier or regressor can significantly affect the model's performance. Different distance metrics measure the similarity or dissimilarity between data points in various ways, and the choice should be made based on the characteristics of your data and the nature of your problem. Here's how the choice of distance metric can impact performance and when you might prefer one metric over the other:\n",
    "\n",
    "Euclidean Distance (L2 Norm):\n",
    "\n",
    "Euclidean distance considers both the magnitude and direction of differences between data points. It calculates the straight-line distance between points in a Euclidean space.\n",
    "It is suitable when features have similar units and scales. However, it can be sensitive to outliers and noisy data.\n",
    "Euclidean distance tends to work well when the underlying data distribution is isotropic (uniform in all directions) and clusters are roughly spherical or circular.\n",
    "When to Choose Euclidean Distance:\n",
    "\n",
    "Data features have similar scales.\n",
    "The data distribution is roughly spherical or isotropic.\n",
    "Outliers are not a significant concern.\n",
    "Manhattan Distance (L1 Norm or City Block Distance):\n",
    "\n",
    "Manhattan distance measures the distance between data points by summing the absolute differences of their coordinates.\n",
    "It is less sensitive to the scale of individual features and can perform better when features have different units or scales.\n",
    "Manhattan distance can be more robust to outliers compared to Euclidean distance.\n",
    "It tends to create square or hyper-rectangular decision boundaries, which can be advantageous when the data distribution is elongated or has irregular shapes.\n",
    "When to Choose Manhattan Distance:\n",
    "\n",
    "Data features have different units or scales.\n",
    "The data distribution is elongated or has irregular shapes.\n",
    "You want a distance metric that is less sensitive to outliers.\n",
    "Minkowski Distance (Generalization of Euclidean and Manhattan Distances):\n",
    "\n",
    "Minkowski distance is a generalized distance metric that includes both Euclidean and Manhattan distances as special cases.\n",
    "It has a parameter 'p' that allows you to control the degree of emphasis on different aspects of distance calculation. When p=1, it is equivalent to Manhattan distance, and when p=2, it is equivalent to Euclidean distance.\n",
    "When to Choose Minkowski Distance:\n",
    "\n",
    "Minkowski distance can be useful when you want to experiment with different distance metrics and tune the 'p' parameter to find the best fit for your data.\n",
    "Other Distance Metrics (e.g., Cosine Similarity, Hamming Distance):\n",
    "\n",
    "Depending on the nature of your data and problem, you may also consider alternative distance metrics like cosine similarity for text data or Hamming distance for binary data.\n",
    "These metrics are specialized for specific types of data and similarity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6353e856",
   "metadata": {},
   "source": [
    "#### Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99691e17",
   "metadata": {},
   "source": [
    "- Common type of hyperparameters in KNN classifiers and regressors are - \n",
    "1. P-value --> This is the most important parameter to choose the method between Euclidean and Manhattan distance,here the default is 'Euclidean distance' with p-value = 2. \n",
    "2. algorithm --> This is also the most important parameter to choose the algorithm between kd-tree,ball tree and auto,here the default is auto. \n",
    "3. weights --> This is the third most important factor to choose the weights as uniform or distance which is used in prediction and by default weights='uniform'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce97300",
   "metadata": {},
   "source": [
    "#### Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892af8f",
   "metadata": {},
   "source": [
    "- The size of the training set for a K-nearest neighbors (KNN) classifier or regressor affects model performance. Smaller sets may lead to overfitting, while larger sets can improve generalization. Optimize training set size using cross-validation, learning curves, data augmentation, feature selection, bootstrapping, or active learning to find the right balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302d4b4",
   "metadata": {},
   "source": [
    "#### Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7ec06",
   "metadata": {},
   "source": [
    "-  Potential drawbacks of K-nearest neighbors (KNN) include computational complexity, sensitivity to feature scaling, choosing the right K value, susceptibility to outliers, memory usage, issues with imbalanced data, the curse of dimensionality, and prediction speed. To mitigate these issues, use techniques such as feature scaling, cross-validation for K selection, outlier handling, memory-efficient data structures, and consider alternative algorithms for high-dimensional data or large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
