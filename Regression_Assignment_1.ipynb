{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004bc70d",
   "metadata": {},
   "source": [
    "# Regression Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35382dd6",
   "metadata": {},
   "source": [
    "**Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba95bf",
   "metadata": {},
   "source": [
    "1. In simple linear regression,there is only one i/p and one o/p and in multiple linear regression we have multiple inputs and a single output. \n",
    "2. In simple linear regression,we will try to find best fit line and in multiple linear regression we will try to find best fit plane and project the points on it. \n",
    "3. Example of simple linear regression is, based on the study hours,we will try to predict the result he/she will get in that particular semester,in which we have a single input and single output.  \n",
    "4. Example of multiple linear regression is, based on the result of last 4 semesters,we will try to predict the result of the current semester,in which we have multiple inputs and a single output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7c396",
   "metadata": {},
   "source": [
    "**Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9129a9",
   "metadata": {},
   "source": [
    "- The assumptions of linear regression are:\n",
    "\n",
    "1. Linearity: The relationship between the independent variable(s) and the dependent variable is linear. This can be checked by plotting the data and looking for a straight-line relationship.\n",
    "2. Homoscedasticity: The variance of the residuals is constant across all values of the independent variable(s). This can be checked by plotting the residuals against the predicted values and looking for a constant variance.\n",
    "3. Normality: The residuals are normally distributed. This can be checked by plotting the residuals in a histogram and looking for a bell-shaped distribution.\n",
    "4. Independence: The residuals are independent of each other. This can be checked by plotting the residuals against the order of the data points and looking for any patterns.\n",
    "5. No multicollinearity: The independent variables are not highly correlated with each other. This can be checked by calculating the correlation coefficients between the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f347b0b",
   "metadata": {},
   "source": [
    "**Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09689cb",
   "metadata": {},
   "source": [
    "1. Intercept can be interpreted by observing when the x-axis is 0,where the y-axis is. \n",
    "2. Slope can be interpreted by observing 'with the unit moment on x-axis how many unit moment on y-axis is there'. \n",
    "3. e.g. Based on the past sales of data,a financial company can use linear regression to find out what will the sales of data in next month. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783efff",
   "metadata": {},
   "source": [
    "**Q4. Explain the concept of gradient descent. How is it used in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6618bb",
   "metadata": {},
   "source": [
    "1. Gradient descent is an optimization algorithm used to find the minimum of a function. It works by starting at a point and then iteratively moving in the direction of the steepest descent until it reaches a minimum.\n",
    "2. In machine learning, gradient descent is used to train machine learning models. The goal of machine learning is to find the parameters of a model that minimize a cost function. The cost function measures how well the model fits the data. Gradient descent can be used to find the parameters that minimize the cost function.\n",
    "3. There are different types of gradient descent algorithms, but they all work in the same basic way. The algorithm starts with an initial guess for the parameters of the model. Then, it calculates the gradient of the cost function at the current point. The gradient is a vector that points in the direction of the steepest descent. The algorithm then moves in the direction of the gradient, and repeats this process until it converges to a minimum.\n",
    "4. The speed of convergence of gradient descent depends on the learning rate. The learning rate is a parameter that controls how much the algorithm moves in the direction of the gradient. A larger learning rate will cause the algorithm to converge faster, but it may also cause the algorithm to overshoot the minimum. A smaller learning rate will cause the algorithm to converge more slowly, but it is less likely to overshoot the minimum.\n",
    "5. Linear regression: Gradient descent can be used to train a linear regression model. In linear regression, the goal is to find a line that best fits the data. Gradient descent can be used to find the parameters of the line that minimize the error between the line and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e13389",
   "metadata": {},
   "source": [
    "**Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea752ea",
   "metadata": {},
   "source": [
    "- In multiple regression model,we have multiple inputs and a single output and in this we will try to find the best fit plane to fit the data and project the points on it to predict an output,whereas in simple linear regression we have single input and a single output and in this we will try to find the best fit line. \n",
    "- Multiple regression model is complex than simple linear regression model but it also can be more accurate if the independent variables are correlated with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a52061",
   "metadata": {},
   "source": [
    "**Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3b225",
   "metadata": {},
   "source": [
    "- Multicollinearity is a statistical phenomenon that occurs when two or more independent variables in a multiple regression model are highly correlated with each other. This can create challenges in the regression analysis because it becomes difficult to determine the individual effects of each independent variable on the dependent variable accurately.\n",
    "\n",
    "- There are a few ways to detect multicollinearity in multiple linear regression:\n",
    "1. Variance Inflation Factor (VIF): The VIF is a measure of how much the variance of an independent variable is inflated due to the presence of multicollinearity. A VIF value greater than 10 is often considered to be a sign of multicollinearity.\n",
    "2. Condition Index: The condition index is a measure of how sensitive the coefficients of a multiple linear regression model are to changes in the independent variables. A condition index greater than 30 is often considered to be a sign of multicollinearity.\n",
    "3. Pearson correlation coefficients: The Pearson correlation coefficients between the independent variables can be used to detect multicollinearity. If two independent variables are highly correlated, then there is multicollinearity.\n",
    "\n",
    "- Once multicollinearity has been detected, there are a few ways to address it:\n",
    "1. Remove one of the correlated independent variables: This is the most effective way to address multicollinearity, but it is not always possible.\n",
    "2. Combine the correlated independent variables: This can be done by creating a new independent variable that is a linear combination of the correlated independent variables.\n",
    "3. Use a different regression method: There are some regression methods that are less sensitive to multicollinearity, such as ridge regression and elastic net regression.\n",
    "- Multicollinearity can make it difficult to interpret the results of the regression model and to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ac45a",
   "metadata": {},
   "source": [
    "**Q7. Describe the polynomial regression model. How is it different from linear regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bea19a",
   "metadata": {},
   "source": [
    "- Polynomial regression model depends on polynomial degrees. \n",
    "- It is similar to linear regression model when polynomial degree is equals to one. \n",
    "- In polynomial regression model we try to find best fit parabola to predict output based on polynomial degree,whereas in linear regression model we try to find best fit line to predict output as it has only one input and only one output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48788138",
   "metadata": {},
   "source": [
    "**Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d00ac",
   "metadata": {},
   "source": [
    "- The advantages and disadvantages of polynomial regression compared to linear regression:\n",
    "\n",
    "- Advantages of polynomial regression over linear regression:\n",
    "\n",
    "1. Polynomial regression can fit a wider range of data than linear regression.\n",
    "2. Polynomial regression can be used to model non-linear relationships.\n",
    "3. Polynomial regression can be more accurate than linear regression if the relationship between the independent and dependent variables is non-linear.\n",
    "- Disadvantages of polynomial regression over linear regression:\n",
    "\n",
    "1. Polynomial regression is more complex than linear regression.\n",
    "2. Polynomial regression is more sensitive to outliers.\n",
    "3. Polynomial regression can be difficult to interpret if the degree of the polynomial is high.\n",
    "- In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "1. You would prefer to use polynomial regression if the relationship between the independent and dependent variables is non-linear. For example, if you are trying to predict the price of a house based on its square footage, you would use polynomial regression because the relationship between price and square footage is not linear.\n",
    "\n",
    "2. You would also prefer to use polynomial regression if you need to fit a wider range of data than linear regression can fit. For example, if you are trying to predict the height of a plant based on its age, you would use polynomial regression because the relationship between height and age is not linear and can vary depending on the plant species.\n",
    "- Disadvantages of polynomial regression are that it is, Polynomial regression is more complex than linear regression and can be more sensitive to outliers. You should also be careful not to choose a degree of polynomial that is too high, as this can lead to overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba180f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
