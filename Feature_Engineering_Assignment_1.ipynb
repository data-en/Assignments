{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b3b5b9",
   "metadata": {},
   "source": [
    "**Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe4c89",
   "metadata": {},
   "source": [
    "1. When we don't have values stored for certain variables or the data is not putten by some people because they want to disclose that data.e.g.,Men don't want to disclose their salary and women don't want to disclose their ages in surveys. \n",
    "2. Missing values needs to be handled because it may affect results and if we want to visualize data,it may also affect the visualisation of data. \n",
    "3. Algorithms that are not affected by missing values are: \n",
    "    1. KNN(K Nearest Neighors)\n",
    "    2. Random Forest\n",
    "    3. Naive Bayes\n",
    "    4. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00464f9",
   "metadata": {},
   "source": [
    "**Q2: List down techniques used to handle missing data. Give an example of each with python code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162067fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age\n",
      "0  18.0\n",
      "1  25.0\n",
      "2  30.0\n",
      "3  29.5\n",
      "4  45.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Mean-Value Imputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({'Age':[18,25,30,np.nan,45]})\n",
    "df['Age'].fillna(df['Age'].mean(),inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2599b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of Salary dataset : 125000.0\n",
      "     Salary\n",
      "0   30000.0\n",
      "1   45000.0\n",
      "2  100000.0\n",
      "3  125000.0\n",
      "4  150000.0\n",
      "5  125000.0\n",
      "6  250000.0\n",
      "7  800000.0\n"
     ]
    }
   ],
   "source": [
    "# 2. Median-Value Imputation \n",
    "# This technique is used when there is outliers present in dataset i.e. if the dataset is right-skewed or left-skewed,this technique is used\n",
    "df = pd.DataFrame({'Salary':[30000,45000,100000,np.nan,150000,np.nan,250000,800000]})\n",
    "df['Salary'].fillna(df['Salary'].median(),inplace=True)\n",
    "print('Median of Salary dataset : {}'.format(df['Salary'].median()))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d408d962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Grades\n",
      "0      A\n",
      "1      B\n",
      "2      C\n",
      "3      B\n",
      "4      B\n",
      "5      B\n",
      "6      A\n",
      "7      B\n",
      "8      B\n",
      "9      B\n"
     ]
    }
   ],
   "source": [
    "# Mode Imputation Technique\n",
    "# This technique is used with categorical variables\n",
    "df_cat = pd.DataFrame({'Grades':['A','B','C',np.nan,'B','B','A',np.nan,'B','B']})\n",
    "df_cat['Grades'].fillna(df_cat['Grades'].mode()[0],inplace=True)\n",
    "print(df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a05ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Salary\n",
      "0   30000.0\n",
      "1   45000.0\n",
      "2  100000.0\n",
      "3  800000.0\n",
      "4  150000.0\n",
      "5  800000.0\n",
      "6  250000.0\n",
      "7  800000.0\n"
     ]
    }
   ],
   "source": [
    "# Random Sampling Imputation technique\n",
    "df = pd.DataFrame({'Salary':[30000,45000,100000,np.nan,150000,np.nan,250000,800000]})\n",
    "df['Salary'].fillna(800000,inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfc6a3",
   "metadata": {},
   "source": [
    "**Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7e358",
   "metadata": {},
   "source": [
    "1. Imbalanced data means the data which have missing values or some irrevant data \n",
    "2. Imbalanced data needs to be handled because it may affect the result and also it affects if we try to visualize data\n",
    "3. There are multiple techniques for handling imbalanced data\n",
    "    1. Upscaling \n",
    "    2. Downscaling \n",
    "    3. SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec058c",
   "metadata": {},
   "source": [
    "**Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50736624",
   "metadata": {},
   "source": [
    "1. Upscaling - It is used to increase the no. of data points in minority to make the distribution balanced \n",
    "2. Downscaling - It is used to decrease the no. of data points in majority to make the distribution balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa9e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# set the random seed for reproducibility \n",
    "np.random.seed(123)\n",
    "\n",
    "# Create a dataframe with two classes \n",
    "n_samples = 500 # total samples \n",
    "class_0_ratio = 0.8\n",
    "n_class_0 = int(n_samples*class_0_ratio) #no.of datapoints in class 0\n",
    "n_class_1 = n_samples-n_class_0 # no. of datapoints in class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "386120ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_class_0,n_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1825a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A DATAFRAME WITH IMBALANCED DATASET\n",
    "class_0 = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(loc=0, scale=1, size=n_class_0),\n",
    "    'feature_2': np.random.normal(loc=0, scale=1, size=n_class_0),\n",
    "    'target': [0] * n_class_0\n",
    "})\n",
    "\n",
    "class_1 = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(loc=2, scale=1, size=n_class_1),\n",
    "    'feature_2': np.random.normal(loc=2, scale=1, size=n_class_1),\n",
    "    'target': [1] * n_class_1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db21beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([class_0,class_1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa03b3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.085631</td>\n",
       "      <td>1.534090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997345</td>\n",
       "      <td>-0.529914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.282978</td>\n",
       "      <td>-0.490972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.506295</td>\n",
       "      <td>-1.309165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.578600</td>\n",
       "      <td>-0.008660</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  target\n",
       "0  -1.085631   1.534090       0\n",
       "1   0.997345  -0.529914       0\n",
       "2   0.282978  -0.490972       0\n",
       "3  -1.506295  -1.309165       0\n",
       "4  -0.578600  -0.008660       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d87d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    400\n",
       "1    100\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_countsunts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbbf4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upscaling \n",
    "df_minority = df[df['target']==1]\n",
    "df_majority = df[df['target']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9343bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample \n",
    "df_minority_upsampled = resample(df_minority,replace=True,n_samples=len(df_majority),random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0820c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled = pd.concat([df_majority,df_minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e302272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    400\n",
       "1    400\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_upsampled['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "660436c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    400\n",
       "1    100\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c3ad15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downscaling \n",
    "from sklearn.utils import resample\n",
    "df_minority = df[df['target']==1]\n",
    "df_majority = df[df['target']==0]\n",
    "df_majority_downsampled = resample(df_majority,replace=False,n_samples=len(df_minority),random_state=42)\n",
    "df_downsampled = pd.concat([df_minority,df_majority_downsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61ac1d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    100\n",
       "0    100\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downsampled['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8f2ef",
   "metadata": {},
   "source": [
    "**Q5: What is data Augmentation? Explain SMOTE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354b162",
   "metadata": {},
   "source": [
    "1. Data augmentation is a technique used to artificially increase the size of a dataset by creating new data points from existing data points. This can be useful for machine learning tasks where there is a limited amount of data available, or where the data is imbalanced.\n",
    "2. Tne example of data augmentation is SMOTE (Synthetic Minority Over-sampling Technique). SMOTE is a technique for oversampling minority classes in imbalanced datasets. It works by creating new data points that are similar to existing minority class data points. Specifically, SMOTE creates new data points by taking a minority class data point and creating a new data point that is located halfway between the minority class data point and one of its k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b434fde",
   "metadata": {},
   "source": [
    "**Q6: What are outliers in a dataset? Why is it essential to handle outliers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c2c20",
   "metadata": {},
   "source": [
    "1. Outliers are the points that are too big or too low from the rest of the dataset. \n",
    "2. Handling outliers is essential because it affects the result and when we draw a boxplot,it shows the outliers. \n",
    "3. By using five number summary,we can remove outliers,it incldes min value,Q1,median,Q3,max value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71476887",
   "metadata": {},
   "source": [
    "**Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a0772",
   "metadata": {},
   "source": [
    "1. If I am working on a project that requires analyzing customer data and some of the data is missing ,then some of the techniques that I will use to handle missing values are : \n",
    "    1. Mean value Imputation technique - replace missing values using mean\n",
    "    2. Median value Imputation technique - replace missing values using median\n",
    "    3. Mode value Imputation technique - replace missing values using mode\n",
    "    4. Random sampling Imputation technique - replace missing values using any random value of our choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037472d",
   "metadata": {},
   "source": [
    "**Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1fdee",
   "metadata": {},
   "source": [
    "> I will use some of the methods of statistical analysis like hypothesis testing in which we will decide that the null hypothesis should be rejected or accepted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3ad40",
   "metadata": {},
   "source": [
    "**Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9678890",
   "metadata": {},
   "source": [
    "> I will use some techniques that will help me to balance the dataset like imputation techniques I may use there,also I may use f1 score to find the relation of overall dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b90f7",
   "metadata": {},
   "source": [
    "**Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a443f",
   "metadata": {},
   "source": [
    "> The method I will employ to balance the dataset and downsample the majority class is downsampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf9814",
   "metadata": {},
   "source": [
    "**Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5de8f0",
   "metadata": {},
   "source": [
    "> Upscaling or oversampling are the two methods I will try to employ to balance the dataset and upsample the minority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc077fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
